---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  chart:
    spec:
      chart: rook-ceph-cluster
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
      version: v1.14.9
  dependsOn:
    - name: rook-ceph
  interval: 1h
  postRenderers:
    - kustomize:
        patches:
          - target:
              group: monitoring.coreos.com
              version: v1
              kind: PrometheusRule
              name: prometheus-ceph-rules
            patch: |
              # CephNodeDiskspaceWarning
              - op: remove
                path: /spec/groups/6/rules/4
              # CephNodeNetworkPacketDrops
              - op: remove
                path: /spec/groups/6/rules/1
  values:
    # https://github.com/rook/rook/blob/release-1.14/deploy/charts/rook-ceph-cluster/values.yaml
    cephBlockPools:
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: []
          allowedTopologies: []
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
    cephClusterSpec:
      # for MONTHS since first deploying Ceph, I woke up just about every morning to a fresh round
      # of alerts, because some osds have crashed overnight. still no idea why. it still happens today.
      # but it always happens in the dead of night and never seems to cause enough impact for me to care.
      crashCollector:
        disable: true
      dashboard:
        ssl: false
      mon:
        count: 3
      placement:
        mgr:
          nodeAffinity: &cpaff
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/control-plane
                      operator: Exists
          tolerations: &cptol
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  ceph_daemon_type: mgr
                  rook_cluster: rook-ceph
        mon:
          nodeAffinity: *cpaff
          tolerations: *cptol
      removeOSDsIfOutAndSafeToRemove: true
      resources:
        cleanup: &resources
          limits:
            cpu: ~
          requests:
            cpu: ~
        crashcollector: *resources
        logcollector:
          limits:
            cpu: ~
            memory: 512Mi
          requests:
            cpu: ~
            memory: 512Mi
        mgr: *resources
        mgr-sidecar: *resources
        mon: *resources
        osd:
          limits:
            cpu: ~
            memory: 4Gi
          requests:
            cpu: ~
            memory: 2Gi
        prepareosd: *resources
      storage:
        useAllDevices: false
        useAllNodes: false
        config:
          encryptedDevice: "true"
        nodes:
          - name: az1-2
            devices:
              - name: nvme0n1
          - name: az1-3
            devices:
              - name: nvme0n1
          - name: az2-1
            devices:
              - name: nvme0n1
          - name: az3-1
            devices:
              - name: nvme0n1
          - name: az3-2
            devices:
              - name: nvme0n1
    cephFileSystems:
      - name: ceph-filesystem
        spec:
          metadataPool:
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              replicated:
                size: 3
              name: data0
            - failureDomain: host
              erasureCoded:
                dataChunks: 2
                codingChunks: 1
              name: ec
          metadataServer:
            activeCount: 1
            activeStandby: true
            placement:
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
              topologySpreadConstraints:
                - maxSkew: 1
                  topologyKey: topology.kubernetes.io/zone
                  whenUnsatisfiable: DoNotSchedule
                  labelSelector:
                    matchLabels:
                      ceph_daemon_type: mds
                      rook_cluster: rook-ceph
                      rook_file_system: ceph-filesystem
            resources:
              limits:
                cpu: ~
                memory: 4Gi
              requests:
                cpu: ~
                memory: 1Gi
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: ceph-filesystem
          pool: data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: []
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephFileSystemVolumeSnapshotClass:
      enabled: true
    cephObjectStores:
      - name: ceph-objectstore
        ingress:
          enabled: true
          annotations:
            cert-manager.io/cluster-issuer: letsencrypt
            nginx.ingress.kubernetes.io/proxy-body-size: "0"
            nginx.ingress.kubernetes.io/proxy-buffering: "off"
            nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
            nginx.ingress.kubernetes.io/proxy-send-timeout: "300s"
            nginx.ingress.kubernetes.io/proxy-read-timeout: "300s"
          host:
            name: obj.samcday.com
          ingressClassName: nginx
          tls:
            - hosts: [obj.samcday.com]
              secretName: rgw-cert
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 3
          dataPool:
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          preservePoolsOnDelete: true
          gateway:
            port: 80
            resources:
              limits:
                cpu: ~
                memory: 2Gi
              requests:
                cpu: ~
                memory: 1Gi
            instances: 2
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          name: ceph-bucket
          reclaimPolicy: Delete
          volumeBindingMode: Immediate
          parameters:
            region: us-east-1
    ingress:
      dashboard:
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt
        host:
          name: ceph-dashboard.samcday.com
        tls:
          - hosts: [ceph-dashboard.samcday.com]
            secretName: dashboard-certificate
        ingressClassName: nginx
    monitoring:
      enabled: true
      createPrometheusRules: true
    toolbox:
      enabled: true
